{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proj_2:Web  Scrapping BBC Weather Data using Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5WY4UPjfrw291LKKEal+U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shauryaraj5/Web-Scrapping-BBC-Weather-Data_py/blob/main/Proj_2_Web_Scrapping_BBC_Weather_Data_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary Libraries"
      ],
      "metadata": {
        "id": "oAiiJTdj11uj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "849T5VyS1qHL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get webpage from server"
      ],
      "metadata": {
        "id": "atZvSlYA2TVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "required_city = \"Mumbai\"\n",
        "location_url = 'https://locator-service.api.bbci.co.uk/locations?' + urlencode({\n",
        "   'api_key' : 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
        "   'stack' : 'aws',\n",
        "   'locale' : 'en',\n",
        "   'filter' : 'international',\n",
        "   'place-types' : 'settlement,airport,district',\n",
        "   'order':'importance',\n",
        "   's' : required_city,\n",
        "   'a' : 'true',\n",
        "   'format' : 'json'\n",
        "})\n",
        "print(location_url)"
      ],
      "metadata": {
        "id": "u-izl7dM2WGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = requests.get(location_url).json()"
      ],
      "metadata": {
        "id": "_2zKRIku3n8E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.bbc.com/weather/'+result['response']['results']['results'][0]['id']\n",
        "webpage = requests.get(url)"
      ],
      "metadata": {
        "id": "cmUy5HDQ51Ks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an instance of beautifulsoup"
      ],
      "metadata": {
        "id": "kANmTJau7z5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = bs(webpage.content, 'html.parser')\n",
        "content = soup.prettify()\n",
        "print(content)"
      ],
      "metadata": {
        "id": "UYqOP6sd73tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be scraping all the high and low temp values with the summary."
      ],
      "metadata": {
        "id": "gYMKIT-H9oan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_ht = soup.find_all('span', class_='wr-day-temperature__high-value')\n",
        "print(daily_ht)"
      ],
      "metadata": {
        "id": "r-11-s6M9vu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_lt = soup.find_all('span', attrs={'class' : 'wr-day-temperature__low-value'})\n",
        "print(daily_lt)"
      ],
      "metadata": {
        "id": "jxoELRRt_KJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_sum = soup.find('div', class_='wr-day-summary')\n",
        "print(daily_sum)"
      ],
      "metadata": {
        "id": "KEfTws_7_0UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will be post processing the data and extract relevant information like just the temperatures and we will remove all the html parts."
      ],
      "metadata": {
        "id": "ZuUYfZORIVA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a list of all High temp values"
      ],
      "metadata": {
        "id": "YShyw6uuJk5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_ht[0].text.strip()\n",
        "daily_ht[0].text.strip().split()\n",
        "daily_ht[0].text.strip().split()[0]"
      ],
      "metadata": {
        "id": "GLnozgZYIhrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_ht_list = [daily_ht[i].text.strip().split()[0] for i in range(len(daily_ht))]\n",
        "print(daily_ht_list)"
      ],
      "metadata": {
        "id": "6SAKTVX6I9-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating list of all low temp values"
      ],
      "metadata": {
        "id": "oLfHRFq0Jt6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_lt_list = [daily_lt[i].text.strip().split()[0] for i in range(len(daily_lt))]\n",
        "print(daily_lt_list)"
      ],
      "metadata": {
        "id": "O79AMr2MJ7ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will be getting list of summaries.\n",
        "since summary is just a long text with new descriptions starting with n upper case letter, we will use the upper case letter as the separator"
      ],
      "metadata": {
        "id": "0KXE6qNGKNzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_sum.text"
      ],
      "metadata": {
        "id": "fbmDx48sKdp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = \"\"\n",
        "for i in daily_sum.text:\n",
        "  if i.isupper():\n",
        "    res += \"*\"+i\n",
        "  else:\n",
        "    res += i\n",
        "daily_sum_list = res.split(\"*\")\n",
        "daily_sum_list.remove('')\n",
        "print(daily_sum_list)"
      ],
      "metadata": {
        "id": "sOguwzWeKhhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist = pd.date_range(datetime.today(), periods = len(daily_ht)).tolist()\n",
        "print(datelist)"
      ],
      "metadata": {
        "id": "U3HP2LFz6ypI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist = [datelist[i].date().strftime('%y-%m-%d') for i in range(len(datelist))]\n",
        "print(datelist)"
      ],
      "metadata": {
        "id": "ddsDbF267uHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = zip(datelist, daily_ht_list, daily_lt_list, daily_sum_list)\n"
      ],
      "metadata": {
        "id": "1Qj99izC9_C4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zipped), columns=['Date', 'Highest', 'Lowest', 'Weather Summary'])"
      ],
      "metadata": {
        "id": "BxWhP94S-ggA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "CHJSXlza-1so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save File as csv and xlsx"
      ],
      "metadata": {
        "id": "La0JWxv-AUEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_csv = required_city+'weather data'+'.csv'\n",
        "df.to_csv(file_csv, index = None)"
      ],
      "metadata": {
        "id": "W4REBJwxAXKt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_xlsx = required_city+'weather data'+'.xlsx'\n",
        "df.to_excel(file_xlsx)"
      ],
      "metadata": {
        "id": "qKKAyuHlA1u4"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}